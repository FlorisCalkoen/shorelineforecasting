{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shoreline Forecasting Data Analysis Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from preprocessing.helpers import (\n",
    "    str2int,\n",
    "    optimize,\n",
    "    unnesting,\n",
    "    pivot_tsdf,\n",
    "    format_tsdf,\n",
    "    add_geometry, \n",
    "    interpolate_nans,  \n",
    "    split_metadata_tsdf, \n",
    "    drop_non_tokenizable,\n",
    "    create_tokenized_tsdf, \n",
    "    drop_empty_geometries, \n",
    "    save_preprocessed_data, \n",
    "    load_preprocessed_data\n",
    ")\n",
    "from preprocessing.filters import (\n",
    "    filter_tsdf_by_nans,\n",
    "    get_metadata_filter,\n",
    "    filter_tsdf_by_metadata\n",
    ")\n",
    "from models.helpers import (\n",
    "    pt2df,\n",
    "    configure_torch, \n",
    "    get_lstm_configs,\n",
    "    get_scaled_splits,\n",
    "    evaluate_performance,\n",
    "    get_torch_dataloaders\n",
    ")\n",
    "from models.lstm import (\n",
    "    Net,\n",
    "    train_model,\n",
    "    inference_model\n",
    ")\n",
    "from visualization.plots import plot_forecast\n",
    "from utils.configs import get_yaml_configs\n",
    "from utils.logger import get_logger\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configurations\n",
    "Configurations are loaded from the ```configurations/default.yml``` file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "configs = get_yaml_configs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logger\n",
    "Initialize a logger object to keep track of data statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(configs)\n",
    "logger.critical(f\"Configs: {configs.items()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "Data is loaded from this [AWS S3 Cloud Storage bucket](\"https://s3.eu-central-1.amazonaws.com/floris.calkoen.open.data/outliers.csv\"). \n",
    "\n",
    "We will load a csv (sample_data.csv) containing all satellite-derived shoreline position data. This data includes both metadata about the sites and the shoreline positions. \n",
    "\n",
    "Alternatively cleaned metadata (sites.csv) and time series (time-series.csv) dataframes can be loaded directly. For demonstration purposes we here continue with the raw sample data.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'data_sample_url'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-9f8371d0bb15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdata\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfigs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data_sample_url'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'data_sample_url'"
     ]
    }
   ],
   "source": [
    "data  = pd.read_csv(configs['data']['data_sample_url'])\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we also load a file (outliers.csv) containing information about outliers which were detected by Luijendijk et al. (2018) and later by Kras (2019). The outlier indices are nested in the csv. We therefore first have to explode the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers = pd.read_csv(configs['data']['outliers_url'])\n",
    "outliers['outliers_1_as_int'] = outliers['outliers_1'].apply(str2int)\n",
    "outliers['outliers_2_as_int'] = outliers['outliers_2'].apply(str2int)\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "First we optimize the dtypes of the dataframe to reduce its size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = optimize(data, ignore_features = ['dt', 'dist', 'outliers_1', 'outliers_2'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will seperate the time-serise data and metadata. First we create a tokenized version of the dataframe; we split the shoreline positions (distance) and dates (dt) from one string into floats. We also drop the non-tokenizable strings; the sites without observations. Finally we seperate the data into two dataframes: time series and metadata. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = create_tokenized_tsdf(data)\n",
    "data = drop_non_tokenizable(data)\n",
    "metadata, tsdf = split_metadata_tsdf(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Above we can see that the time series data is now saved in an array. However, both the dates and shoreline positions are still kept in nested data structures. Furthermore, the dates are still expressed in decimals. Here we will unnest the dataframe and change the decimals to datetime objects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf = unnesting(tsdf, explode=['dt', 'dist'])\n",
    "tsdf = format_tsdf(tsdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are working with geospatial data, we will now convert the Pandas DataFrame to a GeoPandas DataFrame. Such dataframe with also include a geometry column. Meanwhile we will drop all sites without coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = add_geometry(metadata)\n",
    "metadata = drop_empty_geometries(metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by metadata\n",
    "Here we filter the data according to configurations set in the configurations- (here default.yml) file. First we create a list of the transects we want to include into the analysis based on criteria such as whether the beach is sandy; or if the sediment is composited. We use this filter to get an updated time-series dataframe. We will furthermore drop both outliers 1 and 2 according to the outliers information provided in the file outliers.csv \n",
    "\n",
    "Finally we will pivot the dataframe so that each of the columns describe shoreline evolution of one of the transects. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_filter = get_metadata_filter(metadata, tsdf, configs)\n",
    "tsdf = filter_tsdf_by_metadata(tsdf, configs, outliers, metadata_filter)\n",
    "tsdf = pivot_tsdf(tsdf)\n",
    "tsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter by Nan's\n",
    "Most transects have missing values for one or more years due to clouds, measurement error's or problems with the satellite. However, to reliably use the data for forecasting we need a certain amount of observations. We therefore only include transects with less than 25% nan's. The remaining nan's will be linearly interpolated in both directions. Finally we will save these preprocessed results priorly to forecasting. Data will be saved in pickle-formate to keep the optimized dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsdf, _, _ = filter_tsdf_by_nans(tsdf, configs)\n",
    "tsdf = interpolate_nans(tsdf)\n",
    "save_preprocessed_data(tsdf, metadata, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forecasting\n",
    "We now focus on forecasting. Here we only present a basic LSTM network. In the future shorelines application will also other algorithms be implemented. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally directly load data when data preprocessed data has been saved.\n",
    "configs, tsdf, metadata = load_preprocessed_data(filename=\"sample_1598259358.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch configurations\n",
    "First we set the basic configurations of PyTorch, such as a random seed for reproducability. We further get the LSTM model configurations.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configure_torch(seed=configs['run']['seed'])\n",
    "model_configs = get_lstm_configs(tsdf, configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split and \n",
    "We first divide the dataset into train, validation and test partitions. This data is scaled between 0 and 1. Please note that scaling is only performed after splitting the data into train and test partitions in order to avoid information leaking into the test set. The model will be trained on the training set. Then the hyperparameters will be optimized by evaluation on the validation set. Finally, the model performance will be evaluated on the test set. Later we have to re-scale the output values, so we also save the scaler. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test, test_raw, test_scaler = get_scaled_splits(tsdf, ratio=model_configs['split_ratio'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input preparation\n",
    "PyTorch includes automatic gradient calculations. However, to use these the data should be hold in PyTorch Tensor's. We additionaly load the tensor's into PyTorch's dataloaders for practical purposes. The data in these dataloaders consists of 10 time series per batch. Furthermore we see that the data is now split into 24:9 proportion: the first 24 years will be used for training, while the 9 latest are used for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = get_torch_dataloaders(train, val, test, train_window=model_configs['train_window'],\n",
    "                                    batch_size=model_configs['batch_size'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RNN Model\n",
    "Here we define a basic RNN network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = Net(model_configs['input_size'], model_configs['hidden_size'], model_configs['output_size'],\n",
    "           model_configs['batch_size'], model_configs['train_window'])\n",
    "print(rnn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train loop\n",
    "We train the model for 10 epochs. This is incredibly fast since we only used a small sample of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(rnn, dataloaders, model_configs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inference\n",
    "We now use the trained model for inference on the test partition. The overall performance is evaluated by several statistics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast = inference_model(rnn, dataloaders['test'], model_configs)\n",
    "forecast = pt2df(forecast, model_configs, test, inverse_scaling=True, test_scaler=test_scaler)\n",
    "performance = forecast.apply(lambda x: evaluate_performance(test_raw[x.name], x, model_configs),\n",
    "                                       result_type=\"expand\").T\n",
    "performance.columns = model_configs['evaluation']\n",
    "performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show a forecast example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecast(test_raw, forecast, configs, model_configs, transect_id=model_configs['transect_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n Performance LSTM: \\n{performance.mean()}\")\n",
    "timestamp = int(datetime.timestamp(datetime.now()))\n",
    "forecast.to_csv(f\"output/forecasts/lstm_{timestamp}.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

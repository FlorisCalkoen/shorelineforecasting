{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import mxnet as mx\n",
    "from mxnet import gluon\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "from utils import GluonConfigs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transects included in dataset: 37111; timesteps: 33\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1984</th>\n",
       "      <th>1985</th>\n",
       "      <th>1986</th>\n",
       "      <th>1987</th>\n",
       "      <th>1988</th>\n",
       "      <th>1989</th>\n",
       "      <th>1990</th>\n",
       "      <th>1991</th>\n",
       "      <th>1992</th>\n",
       "      <th>1993</th>\n",
       "      <th>...</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "      <th>2012</th>\n",
       "      <th>2013</th>\n",
       "      <th>2014</th>\n",
       "      <th>2015</th>\n",
       "      <th>2016</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>transect_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>BOX_051_151_15</th>\n",
       "      <td>677.32043</td>\n",
       "      <td>695.66370</td>\n",
       "      <td>713.29486</td>\n",
       "      <td>682.1743</td>\n",
       "      <td>688.23425</td>\n",
       "      <td>672.13007</td>\n",
       "      <td>700.43286</td>\n",
       "      <td>693.43460</td>\n",
       "      <td>699.38560</td>\n",
       "      <td>687.97950</td>\n",
       "      <td>...</td>\n",
       "      <td>700.6640</td>\n",
       "      <td>712.41740</td>\n",
       "      <td>708.45123</td>\n",
       "      <td>740.33240</td>\n",
       "      <td>680.42510</td>\n",
       "      <td>758.35150</td>\n",
       "      <td>754.49695</td>\n",
       "      <td>763.04297</td>\n",
       "      <td>743.29680</td>\n",
       "      <td>779.41570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOX_051_151_18</th>\n",
       "      <td>768.93800</td>\n",
       "      <td>769.23883</td>\n",
       "      <td>762.44300</td>\n",
       "      <td>755.8622</td>\n",
       "      <td>761.79663</td>\n",
       "      <td>760.51710</td>\n",
       "      <td>763.30505</td>\n",
       "      <td>761.81360</td>\n",
       "      <td>769.22100</td>\n",
       "      <td>765.47060</td>\n",
       "      <td>...</td>\n",
       "      <td>763.1057</td>\n",
       "      <td>759.72600</td>\n",
       "      <td>766.23150</td>\n",
       "      <td>770.77130</td>\n",
       "      <td>798.26830</td>\n",
       "      <td>797.99615</td>\n",
       "      <td>803.90950</td>\n",
       "      <td>797.90780</td>\n",
       "      <td>798.18830</td>\n",
       "      <td>803.68256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOX_051_151_21</th>\n",
       "      <td>711.41626</td>\n",
       "      <td>684.20540</td>\n",
       "      <td>695.49817</td>\n",
       "      <td>701.6695</td>\n",
       "      <td>705.29990</td>\n",
       "      <td>703.61470</td>\n",
       "      <td>707.69403</td>\n",
       "      <td>692.90360</td>\n",
       "      <td>704.68280</td>\n",
       "      <td>704.99945</td>\n",
       "      <td>...</td>\n",
       "      <td>825.8698</td>\n",
       "      <td>820.51720</td>\n",
       "      <td>820.68680</td>\n",
       "      <td>842.18097</td>\n",
       "      <td>850.45460</td>\n",
       "      <td>775.96700</td>\n",
       "      <td>883.04240</td>\n",
       "      <td>867.92426</td>\n",
       "      <td>877.37415</td>\n",
       "      <td>874.50244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOX_051_151_30</th>\n",
       "      <td>795.81573</td>\n",
       "      <td>820.63700</td>\n",
       "      <td>799.72380</td>\n",
       "      <td>799.8664</td>\n",
       "      <td>823.56260</td>\n",
       "      <td>822.07300</td>\n",
       "      <td>823.56800</td>\n",
       "      <td>823.73300</td>\n",
       "      <td>824.58400</td>\n",
       "      <td>824.33360</td>\n",
       "      <td>...</td>\n",
       "      <td>823.7820</td>\n",
       "      <td>823.15674</td>\n",
       "      <td>823.65790</td>\n",
       "      <td>821.31860</td>\n",
       "      <td>818.85803</td>\n",
       "      <td>817.31220</td>\n",
       "      <td>822.92970</td>\n",
       "      <td>818.73350</td>\n",
       "      <td>822.18480</td>\n",
       "      <td>818.73480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>BOX_051_151_32</th>\n",
       "      <td>242.70204</td>\n",
       "      <td>238.05159</td>\n",
       "      <td>229.93718</td>\n",
       "      <td>244.0620</td>\n",
       "      <td>247.68105</td>\n",
       "      <td>257.49900</td>\n",
       "      <td>302.69217</td>\n",
       "      <td>301.27722</td>\n",
       "      <td>301.12033</td>\n",
       "      <td>316.40414</td>\n",
       "      <td>...</td>\n",
       "      <td>301.1231</td>\n",
       "      <td>313.63553</td>\n",
       "      <td>313.09814</td>\n",
       "      <td>312.90347</td>\n",
       "      <td>306.18658</td>\n",
       "      <td>294.18326</td>\n",
       "      <td>297.23654</td>\n",
       "      <td>302.88650</td>\n",
       "      <td>323.85840</td>\n",
       "      <td>304.36823</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     1984       1985       1986      1987       1988  \\\n",
       "transect_id                                                            \n",
       "BOX_051_151_15  677.32043  695.66370  713.29486  682.1743  688.23425   \n",
       "BOX_051_151_18  768.93800  769.23883  762.44300  755.8622  761.79663   \n",
       "BOX_051_151_21  711.41626  684.20540  695.49817  701.6695  705.29990   \n",
       "BOX_051_151_30  795.81573  820.63700  799.72380  799.8664  823.56260   \n",
       "BOX_051_151_32  242.70204  238.05159  229.93718  244.0620  247.68105   \n",
       "\n",
       "                     1989       1990       1991       1992       1993  ...  \\\n",
       "transect_id                                                            ...   \n",
       "BOX_051_151_15  672.13007  700.43286  693.43460  699.38560  687.97950  ...   \n",
       "BOX_051_151_18  760.51710  763.30505  761.81360  769.22100  765.47060  ...   \n",
       "BOX_051_151_21  703.61470  707.69403  692.90360  704.68280  704.99945  ...   \n",
       "BOX_051_151_30  822.07300  823.56800  823.73300  824.58400  824.33360  ...   \n",
       "BOX_051_151_32  257.49900  302.69217  301.27722  301.12033  316.40414  ...   \n",
       "\n",
       "                    2007       2008       2009       2010       2011  \\\n",
       "transect_id                                                            \n",
       "BOX_051_151_15  700.6640  712.41740  708.45123  740.33240  680.42510   \n",
       "BOX_051_151_18  763.1057  759.72600  766.23150  770.77130  798.26830   \n",
       "BOX_051_151_21  825.8698  820.51720  820.68680  842.18097  850.45460   \n",
       "BOX_051_151_30  823.7820  823.15674  823.65790  821.31860  818.85803   \n",
       "BOX_051_151_32  301.1231  313.63553  313.09814  312.90347  306.18658   \n",
       "\n",
       "                     2012       2013       2014       2015       2016  \n",
       "transect_id                                                            \n",
       "BOX_051_151_15  758.35150  754.49695  763.04297  743.29680  779.41570  \n",
       "BOX_051_151_18  797.99615  803.90950  797.90780  798.18830  803.68256  \n",
       "BOX_051_151_21  775.96700  883.04240  867.92426  877.37415  874.50244  \n",
       "BOX_051_151_30  817.31220  822.92970  818.73350  822.18480  818.73480  \n",
       "BOX_051_151_32  294.18326  297.23654  302.88650  323.85840  304.36823  \n",
       "\n",
       "[5 rows x 33 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf = GluonConfigs.load_data()\n",
    "tf = tf.set_index('transect_id')\n",
    "tf = tf.dropna(thresh=33)\n",
    "print(f\"Transects included in dataset: {tf.shape[0]}; timesteps: {tf.shape[1]}\")\n",
    "tf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf = tf.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf = tf.sample(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sites = pd.read_csv(\"/media/storage/data/shorelines/sites-gluonts-prepared.csv\")\n",
    "sites = sites.loc[sites['transect_id'].isin(tf.index)]\n",
    "\n",
    "logs = np.log(sites['changerate_unc'].values)\n",
    "sites['changerate_cat'] = pd.qcut(logs, q=10, labels=range(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = {'num_series': len(tf),\n",
    "            'num_steps': len(tf.columns),\n",
    "            'prediction_length': 7,\n",
    "            'freq': \"AS\",\n",
    "            'start': [pd.Timestamp(\"01-01-1984\", freq='AS') for _ in range(len(tf))],\n",
    "            'item_id': tf.index.values,\n",
    "            'context_length': 26\n",
    "            }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gluonts.dataset.field_names import FieldName\n",
    "from gluonts.dataset.common import ListDataset\n",
    "\n",
    "train_ds = ListDataset(\n",
    "    [\n",
    "     {\n",
    "         FieldName.TARGET: target,\n",
    "         FieldName.START: start,\n",
    "         FieldName.ITEM_ID: item_id,\n",
    "         FieldName.FEAT_STATIC_CAT: [fsc1, fsc2],\n",
    "         FieldName.FEAT_STATIC_REAL: [fsr]\n",
    "      }\n",
    "     \n",
    "     for (target, start, item_id, fsc1, fsc2, fsr) in zip(tf.values[:, :-metadata['prediction_length']],\n",
    "                                        metadata['start'], \n",
    "                                        metadata['item_id'],\n",
    "                                        sites['coastline_idint'].values,\n",
    "                                        sites['changerate_cat'].values,\n",
    "                                        sites['changerate_unc'].values)\n",
    "    ], freq=metadata['freq'])\n",
    "\n",
    "test_ds = ListDataset(\n",
    "    [\n",
    "     {\n",
    "         FieldName.TARGET: target,\n",
    "         FieldName.START: start,\n",
    "         FieldName.ITEM_ID: item_id,\n",
    "         FieldName.FEAT_STATIC_CAT: [fsc1, fsc2],\n",
    "         FieldName.FEAT_STATIC_REAL: [fsr]\n",
    "      }\n",
    "     \n",
    "     for (target, start, item_id, fsc1, fsc2, fsr) in zip(tf.values,\n",
    "                                        metadata['start'], \n",
    "                                        metadata['item_id'],\n",
    "                                        sites['coastline_idint'].values,\n",
    "                                        sites['changerate_cat'].values,\n",
    "                                        sites['changerate_unc'].values)\n",
    "    ], freq=metadata['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evaluating gluonts.model.simple_feedforward._estimator.SimpleFeedForwardEstimator(batch_normalization=False, context_length=26, distr_output=gluonts.distribution.student_t.StudentTOutput(), freq=\"AS\", mean_scaling=True, num_hidden_dimensions=None, num_parallel_samples=100, prediction_length=7, trainer=gluonts.trainer._base.Trainer(batch_size=32, clip_gradient=10.0, ctx=None, epochs=10, hybridize=True, init=\"xavier\", learning_rate=0.001, learning_rate_decay_factor=0.5, minimum_learning_rate=5e-05, num_batches_per_epoch=100, patience=10, weight_decay=1e-08))\n",
      "learning rate from ``lr_scheduler`` has been overwritten by ``learning_rate`` in optimizer.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:00<00:00, 135.20it/s, epoch=1/10, avg_epoch_loss=7.06]\n",
      "100%|██████████| 100/100 [00:00<00:00, 135.74it/s, epoch=2/10, avg_epoch_loss=5.33]\n",
      "100%|██████████| 100/100 [00:00<00:00, 147.95it/s, epoch=3/10, avg_epoch_loss=5.01]\n",
      "100%|██████████| 100/100 [00:00<00:00, 147.14it/s, epoch=4/10, avg_epoch_loss=4.82]\n",
      "100%|██████████| 100/100 [00:00<00:00, 145.24it/s, epoch=5/10, avg_epoch_loss=4.84]\n",
      "100%|██████████| 100/100 [00:00<00:00, 148.29it/s, epoch=6/10, avg_epoch_loss=4.78]\n",
      "100%|██████████| 100/100 [00:00<00:00, 150.22it/s, epoch=7/10, avg_epoch_loss=4.86]\n",
      "100%|██████████| 100/100 [00:00<00:00, 149.80it/s, epoch=8/10, avg_epoch_loss=4.8]\n",
      "100%|██████████| 100/100 [00:00<00:00, 147.81it/s, epoch=9/10, avg_epoch_loss=4.8]\n",
      "100%|██████████| 100/100 [00:00<00:00, 148.04it/s, epoch=10/10, avg_epoch_loss=4.76]\n",
      "Running evaluation:   0%|          | 0/1000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n",
      "superyaya\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running evaluation: 100%|██████████| 1000/1000 [00:01<00:00, 695.46it/s]\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import pprint\n",
    "from functools import partial\n",
    "\n",
    "\n",
    "from gluonts.distribution.piecewise_linear import PiecewiseLinearOutput\n",
    "from gluonts.evaluation import Evaluator\n",
    "from gluonts.evaluation.backtest import make_evaluation_predictions\n",
    "from gluonts.model.deepar import DeepAREstimator\n",
    "from gluonts.model.seq2seq import MQCNNEstimator\n",
    "from gluonts.model.simple_feedforward import SimpleFeedForwardEstimator\n",
    "from gluonts.trainer import Trainer\n",
    "\n",
    "import project_path\n",
    "from utils import get_predictor_id\n",
    "\n",
    "\n",
    "epochs = 10\n",
    "num_batches_per_epoch = 100\n",
    "count = 0\n",
    "\n",
    "estimators = [\n",
    "    partial(\n",
    "        SimpleFeedForwardEstimator,\n",
    "        freq=metadata[\"freq\"],\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "    partial(\n",
    "        DeepAREstimator,\n",
    "        freq=\"12M\",\n",
    "        use_feat_static_real=True,\n",
    "        use_feat_static_cat=True,\n",
    "        cardinality=[len(sites['coastline_idint'].unique())],\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "    partial(\n",
    "        DeepAREstimator,\n",
    "        freq=\"12M\",\n",
    "        use_feat_static_real=True,\n",
    "        use_feat_static_cat=True,\n",
    "        cardinality=[len(sites['coastline_idint'].unique())],\n",
    "        distr_output=PiecewiseLinearOutput(8),\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "    partial(\n",
    "        MQCNNEstimator,\n",
    "        freq=metadata[\"freq\"],\n",
    "        trainer=Trainer(\n",
    "            epochs=epochs, num_batches_per_epoch=num_batches_per_epoch\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "def evaluate(estimator):\n",
    "    estimator = estimator(\n",
    "        prediction_length=metadata['prediction_length'],\n",
    "        context_length=metadata['context_length'],\n",
    "\n",
    "        \n",
    "\n",
    "    )\n",
    "\n",
    "    print(f\"evaluating {estimator}\")\n",
    "\n",
    "    predictor = estimator.train(train_ds)\n",
    "    \n",
    "    print('yay')\n",
    "\n",
    "    forecast_it, ts_it = make_evaluation_predictions(\n",
    "        test_ds, predictor=predictor, num_samples=100\n",
    "    )\n",
    "    \n",
    "    print('superyaya')\n",
    "\n",
    "    agg_metrics, item_metrics = Evaluator()(\n",
    "        ts_it, forecast_it, num_series=len(test_ds)\n",
    "    )\n",
    "    \n",
    "    print(agg_metrics)\n",
    "    print()\n",
    "    \n",
    "    item_metrics[\"predictor_id\"] = get_predictor_id(just_read=True)\n",
    "    item_metrics[\"predictor\"] = type(estimator).__name__\n",
    "    agg_metrics[\"predictor_id\"] = get_predictor_id(just_read=True)\n",
    "    agg_metrics[\"predictor\"] = type(estimator).__name__\n",
    "    return item_metrics, agg_metrics\n",
    "\n",
    "\n",
    "item_metrics_list = []\n",
    "agg_metrics_list = []\n",
    "for estimator in estimators:\n",
    "    count+=1\n",
    "    # catch exceptions that are happening during training to avoid failing the whole evaluation\n",
    "    try:\n",
    "        item_metrics, agg_metrics = evaluate(estimator)\n",
    "\n",
    "        item_metrics_list.append(item_metrics)\n",
    "        agg_metrics_list.append(agg_metrics)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"\\n---> ---> ---> --->\\nException error:\\n\")\n",
    "        print(str(e))\n",
    "        print(\"\\n<--- <--- <--- <---\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat(item_metrics_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
